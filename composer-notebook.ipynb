{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/debugger/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/envs/debugger/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/envs/debugger/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/envs/debugger/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/envs/debugger/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/envs/debugger/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda3/envs/debugger/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/envs/debugger/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/envs/debugger/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/envs/debugger/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/envs/debugger/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/envs/debugger/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy\n",
    "\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow. keras.callbacks import ModelCheckpoint\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "# tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_TEXT = open('lyrics-ds.txt', encoding = 'UTF-8').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mapping of unique chars to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of chars: \n",
      " ['\\n', ' ', '!', '\"', '&', \"'\", ',', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "CHARS = sorted(list(set(RAW_TEXT)))\n",
    "print('List of chars: \\n', CHARS)\n",
    "\n",
    "CHAR_TO_INT = dict((c, i) for i, c in enumerate(CHARS))\n",
    "INT_TO_CHAR = dict((i, c) for i, c in enumerate(CHARS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find out how many distinct characters our dataset has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  1103807\n",
      "Total Vocab:  46\n"
     ]
    }
   ],
   "source": [
    "TOTAL_CHARS = len(RAW_TEXT)\n",
    "VOCAB_SIZE = len(CHARS)\n",
    "\n",
    "print('Total Characters: ', TOTAL_CHARS) # 1.1M\n",
    "print('Total Vocab: ', VOCAB_SIZE) # 46 distinct characters (much more than 26 in the alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters for the learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50     # the number times that the learning algorithm will work through the entire training dataset\n",
    "BATCH_SIZE = 64 # the number of samples to work through before updating the internal model parameters\n",
    "SEQ_LENGTH = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the main functions of AI model lifecycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # prepare the data set of input to output pairs encoded as integers\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, TOTAL_CHARS - SEQ_LENGTH, 1):\n",
    "        seq_in = RAW_TEXT[i:i + SEQ_LENGTH]\n",
    "        seq_out = RAW_TEXT[i + SEQ_LENGTH]\n",
    "        dataX.append([CHAR_TO_INT[char] for char in seq_in])\n",
    "        dataY.append(CHAR_TO_INT[seq_out])\n",
    "    n_patterns = len(dataX)\n",
    "\n",
    "    # An example of a sequence length 3 in a dataset containing the text 'SAMPLE', the first 2 training patterns would be SAM -> P, AMP -> L\n",
    "    #print (\"Total Patterns: \", n_patterns) # a bit under 1.1M (TOTAL_CHARS - SEQ_LENGTH)\n",
    "\n",
    "    # reshape X to be [samples, time steps, features]\n",
    "    X = numpy.reshape(dataX, (n_patterns, SEQ_LENGTH, 1))\n",
    "\n",
    "    # normalize\n",
    "    X = X / float(VOCAB_SIZE)\n",
    "\n",
    "    # one hot encode the output variable\n",
    "    y = utils.to_categorical(dataY)\n",
    "\n",
    "    return X,y\n",
    "\n",
    "def create_model(X, y, layers):\n",
    "    # define the LSTM model\n",
    "    # Problem the model solves: single character classification problem with 46 classes (VOCAB_SIZE)\n",
    "    # There is no test dataset. Model the entire training dataset to learn the probability of each character in a sequence\n",
    "    print(\"Creating model...\")\n",
    "    model = Sequential()\n",
    "    \n",
    "    for n in range(layers-2):\n",
    "        # add hidden LSTM layer with 256 memory units\n",
    "        model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True)) \n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y.shape[1], activation='softmax')) # outputs a probability prediction for each of the 46 characters between 0 and 1\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model\n",
    "\n",
    "def train(X, y, checkpoint_file=None):\n",
    "    model = create_model(X, y)\n",
    "\n",
    "    # use model checkpointing for optimization (too slow to train)\n",
    "    # record all of the network weights to file each time\n",
    "    # an improvement in loss is observed at the end of the epoch\n",
    "    # then the best set of weights (lowest loss) to instantiate the generative model in the next section\n",
    "\n",
    "    # define the checkpoint\n",
    "    # filepath = 'lstm-4-layers-weights-improvement-{epoch:02d}-{loss:.4f}.hdf5'\n",
    "    filepath = 'weights-improvement-{epoch:02d}-{loss:.4f}.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    if checkpoint_file:\n",
    "        print(\"Loading checkpoint: \" + checkpoint_file)\n",
    "        model.load_weights(checkpoint_file)\n",
    "\n",
    "    # Fit model to the data (for now use 50 epochs and a medium batch size of 64 patterns)\n",
    "    model.fit(X, y, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks_list, shuffle=True)\n",
    "\n",
    "def generate_lyric(X, y, weights_file):\n",
    "    layers = int(weights_file[0])\n",
    "    model = create_model(X, y, layers)\n",
    "\n",
    "    # load the network weights\n",
    "    model.load_weights(weights_file)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    n_patterns = TOTAL_CHARS - SEQ_LENGTH\n",
    "    start = numpy.random.randint(0, n_patterns-1)\n",
    "    pattern = [CHAR_TO_INT[char] for char in RAW_TEXT[start:start+SEQ_LENGTH]]\n",
    "    output = [INT_TO_CHAR[value] for value in pattern]\n",
    "\n",
    "#     print('Seed:')\n",
    "#     print(''.join(output))\n",
    "\n",
    "    # generate characters\n",
    "    for i in range(1000):\n",
    "        X = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "        X = X / float(VOCAB_SIZE)\n",
    "        prediction = model.predict(X, verbose=0)\n",
    "        # index = numpy.argmax(prediction)\n",
    "        # index = tf.random.categorical(prediction, 1)[-1,0].numpy()\n",
    "        index = numpy.random.choice(len(prediction[0]), p=prediction[0])\n",
    "        result = INT_TO_CHAR[index]\n",
    "        output.append(result)\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    return ''.join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We are skipping data training for now to use checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate lyric 1\n",
    "### Using a 3-lstem-layer model trained in 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "\n",
      "OUTPUT:\n",
      " e message\n",
      "and the lunatic fears\n",
      "he is no more\n",
      "speak to me\n",
      "it all would be easier\n",
      "while i'm passing blood\n",
      "eornaga i uidy our life\n",
      "far within the facts it's night of touer\n",
      "id you fuer lielt 'caat. fireer is laie tired\n",
      "and it shall\n",
      "be ovr ceaays that lies in micning of all closds\n",
      "wouldst the endless warted weaross orr\n",
      "has done to astitl drysted our wing\n",
      "fqom the vipter tuonid vincre of laws\n",
      "iu's wearing dust cowr, tom pain\n",
      "i ac monn agpny\n",
      "uhere' dom't give up this hades\n",
      "but whine ttrongernidd is meft\n",
      "in elbctiwarions\n",
      "of ny sidd\n",
      "sandlisy yill hase the baai\n",
      "teostality pows\n",
      "loog anrwer to sav jt yith variols troa\n",
      "hone of bloodlagd percprelp\n",
      "the faredn nf monn' fade and tpptire\n",
      "do mook on me weak away\n",
      "far beyond it feel\n",
      "in a buedc, earaasy, a worl heals say hreatet spoebns\n",
      "and thou all tavs tires tife so paressinn\n",
      "the mine's aro oh halr\n",
      "bedined it a sunf uo to tnm' a land of wearing mnuier\n",
      "but no winter fights befins the stnesw\n",
      "tatte my asms\n",
      "to acr of a world blis it taking a rodf\n",
      "it went as weary tlot weap\n",
      "that pearlered phaees viited\n",
      "long to tis hrnwing clvnie, set the gois winl think\"\n",
      "ac\n"
     ]
    }
   ],
   "source": [
    "print('\\nOUTPUT:\\n', generate_lyric(x, y, \"3-layers-weights-improvement-50-1.4956.hdf5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate lyric 2\n",
    "### Using a 5-lstem-layer model trained in 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "\n",
      "OUTPUT:\n",
      "------------------\n",
      "  the engines\n",
      "remove all the wheel blocks there's no time to waste\n",
      "gathering speed as we head down there's now i'll hear\n",
      "nor be the\n",
      "whispered words\n",
      "we will be reftsed tonight\n",
      "has all ever mose is yiere smoky\n",
      "you will cone to athve\n",
      "the price i see me\n",
      "the hravekace is put the masser fortalen\n",
      "forever reborn carkness will say\n",
      "but they remossed\n",
      "by this peace to see?\n",
      "the darkness suarling now nochess\n",
      "wasted in the mast behende, behind why i'll sing on\n",
      "sppn when you befores that carknning once\n",
      "dark is savisued in gatition\n",
      "the wind of solendornat cruel bgainstuine\n",
      "cruelfing lives fron the vay and afhiaai\n",
      "dead and nowering, spirits\n",
      "fly on it in her shothh\n",
      "all\n",
      "has a winter collc we have out cack\n",
      "dividen smeepes ruent from the lneelsaah the wokn\n",
      "so pain, drawy running\n",
      "like now for gormt\n",
      "fuurom cy the burning one\n",
      "the days i coundn't gone\n",
      "strong to see it all i'm now answers on\n",
      "i pus her all prichory ruegtction out my stinps\n",
      "a faces cedale\n",
      "my sins now a fair\n",
      "graces of spatks and out of the univer a vola\n",
      "for unithes, stoeing tasted to hell and remames\n",
      "of your life eades in my mind\n",
      "futuing the waml \n"
     ]
    }
   ],
   "source": [
    "print('\\nOUTPUT:\\n------------------\\n', generate_lyric(x, y, \"5-layers-weights-improvement-20-1.5581.hdf5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that more lstm layers means deeper and more accurate learning (also more time processing the training).\n",
    "Not only this model is learns how to write lyrics, just like humans it also needs to learn how to make sense of letter and come up with meaningful words :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
