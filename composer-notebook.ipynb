{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heavy Metal Composer AI\n",
    "This notebook generates completely original lyrics using Artificial Intelligence, built under a LSTM recurrent neural networks architecture.\n",
    "The dataset includes over 1k songs from a selection of 10 heavy metal artists listed in `dataprocessing.py`.\n",
    "This AI model purpose is natural language generation, therefore its goal is to generate lyrics character by character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy\n",
    "\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow. keras.callbacks import ModelCheckpoint\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "# tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_TEXT = open('lyrics-ds.txt', encoding = 'UTF-8').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mapping of unique chars to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARS = sorted(list(set(RAW_TEXT)))\n",
    "print('List of chars: \\n', CHARS)\n",
    "\n",
    "CHAR_TO_INT = dict((c, i) for i, c in enumerate(CHARS))\n",
    "INT_TO_CHAR = dict((i, c) for i, c in enumerate(CHARS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find out how many distinct characters our dataset has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_CHARS = len(RAW_TEXT)\n",
    "VOCAB_SIZE = len(CHARS)\n",
    "\n",
    "print('Total Characters: ', TOTAL_CHARS) # 1.1M\n",
    "print('Total Vocab: ', VOCAB_SIZE) # 46 distinct characters (much more than 26 in the alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters for the learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50     # the number times that the learning algorithm will work through the entire training dataset\n",
    "BATCH_SIZE = 64 # the number of samples to work through before updating the internal model parameters\n",
    "SEQ_LENGTH = 100\n",
    "print('Sequence length: ', SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define total patterns from dataset\n",
    "Prepare the dataset from input to output pairs encoded as integers.\n",
    "An example of a sequence length=3 in a dataset containing the text 'SAMPLE', the first 2 training patterns would be SAM -> P, AMP -> L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    pattern_range = TOTAL_CHARS - SEQ_LENGTH\n",
    "    for i in range(0, pattern_range):\n",
    "        seq_in = RAW_TEXT[i:i + SEQ_LENGTH]\n",
    "        seq_out = RAW_TEXT[i + SEQ_LENGTH]\n",
    "        dataX.append([CHAR_TO_INT[char] for char in seq_in])\n",
    "        dataY.append(CHAR_TO_INT[seq_out])\n",
    "    n_patterns = len(dataX)\n",
    "\n",
    "    print (\"Total Patterns (TOTAL_CHARS - SEQ_LENGTH): \", n_patterns) # a bit under 1.1M (TOTAL_CHARS - SEQ_LENGTH)\n",
    "\n",
    "    # reshape X to be [samples, time steps, features]\n",
    "    X = numpy.reshape(dataX, (n_patterns, SEQ_LENGTH, 1))\n",
    "\n",
    "    # normalize\n",
    "    X = X / float(VOCAB_SIZE)\n",
    "\n",
    "    # one hot encode the output variable\n",
    "    y = utils.to_categorical(dataY)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the LSTM model\n",
    "This is called from `train()` or `generate_lyric()` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(X, y, layers):\n",
    "    # define the LSTM model\n",
    "    # Problem the model solves: single character classification problem with 46 classes (VOCAB_SIZE)\n",
    "    # There is no test dataset. Model the entire training dataset to learn the probability of each character in a sequence\n",
    "    print(\"Creating model...\")\n",
    "    model = Sequential()\n",
    "    \n",
    "    for n in range(layers-2):\n",
    "        # add hidden LSTM layer with 256 memory units\n",
    "        model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True)) \n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y.shape[1], activation='softmax')) # outputs a probability prediction for each of the 46 characters between 0 and 1\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### We are skipping data training for now to use already trained checkpoint files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, checkpoint_file=None):\n",
    "    model = create_model(X, y)\n",
    "\n",
    "    # use model checkpointing for optimization (too slow to train)\n",
    "    # record all of the network weights to file each time\n",
    "    # an improvement in loss is observed at the end of the epoch\n",
    "    # then the best set of weights (lowest loss) to instantiate the generative model in the next section\n",
    "\n",
    "    # define the checkpoint\n",
    "    # filepath = 'lstm-4-layers-weights-improvement-{epoch:02d}-{loss:.4f}.hdf5'\n",
    "    filepath = 'weights-improvement-{epoch:02d}-{loss:.4f}.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    if checkpoint_file:\n",
    "        print(\"Loading checkpoint: \" + checkpoint_file)\n",
    "        model.load_weights(checkpoint_file)\n",
    "\n",
    "    # Fit model to the data (for now use 50 epochs and a medium batch size of 64 patterns)\n",
    "    model.fit(X, y, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks_list, shuffle=True)\n",
    "\n",
    "def generate_lyric(X, y, weights_file):\n",
    "    layers = int(weights_file[0])\n",
    "    model = create_model(X, y, layers)\n",
    "\n",
    "    # load the network weights\n",
    "    model.load_weights(weights_file)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    n_patterns = TOTAL_CHARS - SEQ_LENGTH\n",
    "    start = numpy.random.randint(0, n_patterns-1)\n",
    "    pattern = [CHAR_TO_INT[char] for char in RAW_TEXT[start:start+SEQ_LENGTH]]\n",
    "    output = [INT_TO_CHAR[value] for value in pattern]\n",
    "\n",
    "#     print('Seed:')\n",
    "#     print(''.join(output))\n",
    "\n",
    "    # generate characters\n",
    "    for i in range(500):\n",
    "        X = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "        X = X / float(VOCAB_SIZE)\n",
    "        prediction = model.predict(X, verbose=0)\n",
    "        # index = numpy.argmax(prediction)\n",
    "        # index = tf.random.categorical(prediction, 1)[-1,0].numpy()\n",
    "        index = numpy.random.choice(len(prediction[0]), p=prediction[0])\n",
    "        result = INT_TO_CHAR[index]\n",
    "        output.append(result)\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    return ''.join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate lyric 1\n",
    "### Using a 3-lstm-layer model trained in 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nOUTPUT 1:\\n----------------------', generate_lyric(x, y, \"3-layers-weights-improvement-50-1.4956.hdf5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate lyric 2\n",
    "### Using a 5-lstm-layer model trained in 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nOUTPUT 2:\\n----------------------\\n', generate_lyric(x,y, \"5-layers-weights-improvement-20-1.5581.hdf5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Note:** \n",
    "More lstm layers means deeper and more accurate learning (also more time processing the training).\n",
    "Not only this model is learns how to write lyrics, just like humans it also needs to learn how to make sense of letter and come up with meaningful words :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
